{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31029c06",
   "metadata": {},
   "source": [
    "**1.** Working with RDDs:\n",
    "\n",
    "a) Write a Python program to create an RDD from a local data source.\n",
    "\n",
    "b) Implement transformations and actions on the RDD to perform data processing tasks.\n",
    "\n",
    "c) Analyze and manipulate data using RDD operations such as map, filter, reduce, or\n",
    "aggregate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a7a32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "rdd = spark.sparkContext.textFile(\"path/to/local/data/file.txt\")\n",
    "\n",
    "# Example usage\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa9857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example transformations\n",
    "filtered_rdd = rdd.filter(lambda line: \"error\" in line)\n",
    "mapped_rdd = rdd.map(lambda line: (line.split()[0], 1))\n",
    "\n",
    "# Example actions\n",
    "count = rdd.count()\n",
    "first_element = rdd.first()\n",
    "\n",
    "# Example chaining transformations and actions\n",
    "result = rdd.filter(lambda line: \"error\" in line).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e5cd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example map operation\n",
    "mapped_rdd = rdd.map(lambda line: line.upper())\n",
    "\n",
    "# Example filter operation\n",
    "filtered_rdd = rdd.filter(lambda line: \"error\" in line)\n",
    "\n",
    "# Example reduce operation\n",
    "total_length = rdd.map(lambda line: len(line)).reduce(lambda a, b: a + b)\n",
    "\n",
    "# Example aggregate operation\n",
    "agg_result = rdd.aggregate(0, lambda a, line: a + len(line), lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9b1660",
   "metadata": {},
   "source": [
    "**2.** Spark DataFrame Operations:\n",
    "    \n",
    "a) Write a Python program to load a CSV file into a Spark DataFrame.\n",
    "\n",
    "b)Perform common DataFrame operations such as filtering, grouping, or joining.\n",
    "\n",
    "c) Apply Spark SQL queries on the DataFrame to extract insights from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d46883b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.read.csv(\"path/to/csv/file.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Example usage\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f12cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example filtering\n",
    "filtered_df = df.filter(df[\"age\"] > 30)\n",
    "\n",
    "# Example grouping\n",
    "grouped_df = df.groupBy(\"department\").agg({\"salary\": \"mean\"})\n",
    "\n",
    "# Example joining\n",
    "joined_df = df.join(department_df, on=\"department\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190370fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register DataFrame as a temporary table\n",
    "df.createOrReplaceTempView(\"employees\")\n",
    "\n",
    "# Example SQL query\n",
    "result = spark.sql(\"SELECT department, AVG(salary) FROM employees GROUP BY department\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a63005",
   "metadata": {},
   "source": [
    "**3.** Spark Streaming:\n",
    "    \n",
    "a) Write a Python program to create a Spark Streaming application.\n",
    "\n",
    "b) Configure the application to consume data from a streaming source (e.g., Kafka or a\n",
    "socket).\n",
    "\n",
    "c) Implement streaming transformations and actions to process and analyze the incoming\n",
    "data stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cc3361",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "ssc = StreamingContext(spark.sparkContext, batchDuration=1)\n",
    "\n",
    "# Example usage\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "lines.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8810b7b3",
   "metadata": {},
   "source": [
    "In the example above, we are consuming data from a socket on localhost and port 9999. You can modify the source based on your requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c2d0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.jars.packages\", \"mysql:mysql-connector-java:8.0.26\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Example usage with MySQL\n",
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/database\") \\\n",
    "    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", \"table\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .load()\n",
    "\n",
    "# Example usage with PostgreSQL\n",
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/database\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .option(\"dbtable\", \"table\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1c098c",
   "metadata": {},
   "source": [
    "**4.** Spark SQL and Data Source Integration:\n",
    "\n",
    "a) Write a Python program to connect Spark with a relational database (e.g., MySQL,\n",
    "PostgreSQL).\n",
    "\n",
    "b)Perform SQL operations on the data stored in the database using Spark SQL.\n",
    "\n",
    "c) Explore the integration capabilities of Spark with other data sources, such as Hadoop\n",
    "Distributed File System (HDFS) or Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7154c8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example streaming transformations and actions\n",
    "filtered_lines = lines.filter(lambda line: \"error\" in line)\n",
    "filtered_lines.pprint()\n",
    "\n",
    "# Example word count\n",
    "word_counts = lines.flatMap(lambda line: line.split(\" \")).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "word_counts.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14c0d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register DataFrame as a temporary table\n",
    "df.createOrReplaceTempView(\"table\")\n",
    "\n",
    "# Example SQL query\n",
    "result = spark.sql(\"SELECT * FROM table WHERE age > 30\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c777de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with HDFS\n",
    "df = spark.read.csv(\"hdfs://localhost:9000/path/to/file.csv\")\n",
    "\n",
    "# Example usage with Amazon S3\n",
    "df = spark.read.csv(\"s3a://bucket-name/path/to/file.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
