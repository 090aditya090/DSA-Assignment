{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9adb50a0",
   "metadata": {},
   "source": [
    "## TOPIC: Data Warehousing Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f9a5c2",
   "metadata": {},
   "source": [
    "**1.** Design a data warehouse schema for a retail company that includes dimension\n",
    "tables for products, customers, and time. Implement the schema using a relational\n",
    "database management system (RDBMS) of your choice."
   ]
  },
  {
   "cell_type": "raw",
   "id": "fba6264f",
   "metadata": {},
   "source": [
    "Insert sample data into the sales fact table:\n",
    "\n",
    "Products Dimension Table:\n",
    "\n",
    "product_id (Primary Key)\n",
    "product_name\n",
    "category\n",
    "brand\n",
    "price\n",
    "Customers Dimension Table:\n",
    "\n",
    "customer_id (Primary Key)\n",
    "customer_name\n",
    "address\n",
    "city\n",
    "state\n",
    "country\n",
    "Time Dimension Table:\n",
    "\n",
    "date_id (Primary Key)\n",
    "date\n",
    "year\n",
    "month\n",
    "day\n",
    "quarter\n",
    "Sales Fact Table:\n",
    "\n",
    "sales_id (Primary Key)\n",
    "product_id (Foreign Key referencing Products Dimension Table)\n",
    "customer_id (Foreign Key referencing Customers Dimension Table)\n",
    "date_id (Foreign Key referencing Time Dimension Table)\n",
    "sales_amount\n",
    "\n",
    "To implement this schema using a relational database management system, follow these steps:\n",
    "\n",
    "Create the dimension tables:\n",
    "\n",
    "CREATE TABLE products_dim (\n",
    "  product_id SERIAL PRIMARY KEY,\n",
    "  product_name VARCHAR(255),\n",
    "  category VARCHAR(255),\n",
    "  brand VARCHAR(255),\n",
    "  price DECIMAL(10, 2)\n",
    ");\n",
    "\n",
    "CREATE TABLE customers_dim (\n",
    "  customer_id SERIAL PRIMARY KEY,\n",
    "  customer_name VARCHAR(255),\n",
    "  address VARCHAR(255),\n",
    "  city VARCHAR(255),\n",
    "  state VARCHAR(255),\n",
    "  country VARCHAR(255)\n",
    ");\n",
    "\n",
    "CREATE TABLE time_dim (\n",
    "  date_id SERIAL PRIMARY KEY,\n",
    "  date DATE,\n",
    "  year INT,\n",
    "  month INT,\n",
    "  day INT,\n",
    "  quarter INT\n",
    ");\n",
    "\n",
    "Create the sales fact table:\n",
    "\n",
    "CREATE TABLE sales_fact (\n",
    "  sales_id SERIAL PRIMARY KEY,\n",
    "  product_id INT REFERENCES products_dim(product_id),\n",
    "  customer_id INT REFERENCES customers_dim(customer_id),\n",
    "  date_id INT REFERENCES time_dim(date_id),\n",
    "  sales_amount DECIMAL(10, 2)\n",
    ");\n",
    "\n",
    "Insert sample data into the dimension tables:\n",
    "\n",
    "INSERT INTO products_dim (product_name, category, brand, price)\n",
    "VALUES\n",
    "  ('Product 1', 'Category 1', 'Brand 1', 10.99),\n",
    "  ('Product 2', 'Category 2', 'Brand 2', 19.99),\n",
    "  ('Product 3', 'Category 1', 'Brand 1', 15.99);\n",
    "\n",
    "INSERT INTO customers_dim (customer_name, address, city, state, country)\n",
    "VALUES\n",
    "  ('Customer 1', 'Address 1', 'City 1', 'State 1', 'Country 1'),\n",
    "  ('Customer 2', 'Address 2', 'City 2', 'State 2', 'Country 2'),\n",
    "  ('Customer 3', 'Address 3', 'City 3', 'State 3', 'Country 3');\n",
    "\n",
    "INSERT INTO time_dim (date, year, month, day, quarter)\n",
    "VALUES\n",
    "  ('2023-01-01', 2023, 1, 1, 1),\n",
    "  ('2023-02-01', 2023, 2, 1, 1),\n",
    "  ('2023-03-01', 2023, 3, 1, 1);\n",
    "\n",
    "Insert sample data into the sales fact table:\n",
    "\n",
    "INSERT INTO sales_fact (product_id, customer_id, date_id, sales_amount)\n",
    "VALUES\n",
    "  (1, 1, 1, 100.00),\n",
    "  (2, 2, 2, 150.00),\n",
    "  (1, 3, I apologize for the incomplete response. Here's the continuation of step 4:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d46440a",
   "metadata": {},
   "source": [
    "**2.** Create a fact table that captures sales data, including product ID, customer ID,\n",
    "date, and sales amount. Populate the fact table with sample data."
   ]
  },
  {
   "cell_type": "raw",
   "id": "00cfd12b",
   "metadata": {},
   "source": [
    "**Create the fact table named \"sales_fact\" with the necessary columns:**\n",
    "\n",
    "CREATE TABLE sales_fact (\n",
    "  sales_id SERIAL PRIMARY KEY,\n",
    "  product_id INT,\n",
    "  customer_id INT,\n",
    "  date DATE,\n",
    "  sales_amount DECIMAL(10, 2)\n",
    ");\n",
    "\n",
    "**Populate the fact table with sample data using the INSERT INTO statement:**\n",
    "\n",
    "INSERT INTO sales_fact (product_id, customer_id, date, sales_amount)\n",
    "VALUES\n",
    "  (1, 1, '2023-01-01', 100.00),\n",
    "  (2, 2, '2023-01-01', 150.00),\n",
    "  (1, 3, '2023-01-02', 200.00);\n",
    "\n",
    "In the above example, we assume that the product ID, customer ID, date, and sales amount are stored in their respective columns in the sales_fact table. Adjust the column names and data types as per your specific requirements.\n",
    "\n",
    "You can continue populating the sales_fact table with additional sample data by extending the VALUES clause in the INSERT INTO statement.\n",
    "\n",
    "By executing these SQL statements, you have created the sales_fact table and inserted sample sales data into it. The fact table is now ready to be used for analysis and reporting in your retail data warehouse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95d0c3c",
   "metadata": {},
   "source": [
    "**3.** Write SQL queries to retrieve sales data from the data warehouse, including\n",
    "aggregations and filtering based on different dimensions."
   ]
  },
  {
   "cell_type": "raw",
   "id": "29cb99a5",
   "metadata": {},
   "source": [
    "Retrieve total sales amount for each product:\n",
    "\n",
    "SELECT product_id, SUM(sales_amount) AS total_sales FROM sales_fact GROUP BY product_id;\n",
    "\n",
    "Retrieve total sales amount for each customer:\n",
    "\n",
    "SELECT customer_id, SUM(sales_amount) AS total_sales FROM sales_fact GROUP BY customer_id;\n",
    "\n",
    "Retrieve total sales amount for each date:\n",
    "\n",
    "SELECT date, SUM(sales_amount) AS total_sales FROM sales_fact GROUP BY date;\n",
    "\n",
    "Retrieve total sales amount for each product and customer combination:\n",
    "\n",
    "SELECT product_id, customer_id, SUM(sales_amount) AS total_sales FROM sales_fact GROUP BY product_id, customer_id;\n",
    "\n",
    "Retrieve total sales amount for a specific product:\n",
    "\n",
    "SELECT SUM(sales_amount) AS total_sales FROM sales_fact WHERE product_id = 1;\n",
    "\n",
    "Retrieve total sales amount for a specific customer:\n",
    "\n",
    "SELECT SUM(sales_amount) AS total_sales FROM sales_fact WHERE customer_id = 1;\n",
    "\n",
    "Retrieve total sales amount for a specific date range:\n",
    "\n",
    "SELECT SUM(sales_amount) AS total_sales FROM sales_fact WHERE date BETWEEN '2023-01-01' AND '2023-01-31';\n",
    "\n",
    "These are just a few examples of SQL queries to retrieve sales data from the data warehouse. You can modify and expand these queries based on your specific requirements, including additional dimensions or applying more complex filtering conditions.\n",
    "\n",
    "Remember to adjust the table and column names in the queries to match your data warehouse schema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005ec1ba",
   "metadata": {},
   "source": [
    "## TOPIC: ETL and Data Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2379ce",
   "metadata": {},
   "source": [
    "**1.** Design an ETL process using a programming language (e.g., Python) to extract\n",
    "data from a source system (e.g., CSV files), transform it by applying certain business\n",
    "rules or calculations, and load it into a data warehouse."
   ]
  },
  {
   "cell_type": "raw",
   "id": "06480476",
   "metadata": {},
   "source": [
    "Designing an ETL Process:\n",
    "\n",
    "To design an ETL process, you need to consider the following steps:\n",
    "Extraction: Identify the source systems from which you want to extract data. This can be CSV files, databases, APIs, or other data sources. Determine the extraction method and specify the data fields or tables to extract.\n",
    "\n",
    "Transformation: Define the transformations and business rules to be applied to the extracted data. This may include cleaning, filtering, aggregating, joining, or applying calculations to the data.\n",
    "\n",
    "Loading: Design the structure and schema of the data warehouse tables. Determine the loading method, such as incremental or full load, and specify how the transformed data will be loaded into the data warehouse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b365a6c8",
   "metadata": {},
   "source": [
    "**2.** Implement the ETL process by writing code that performs the extraction,\n",
    "transformation, and loading steps."
   ]
  },
  {
   "cell_type": "raw",
   "id": "9c35ea3a",
   "metadata": {},
   "source": [
    "Implementing the ETL Process in Python:\n",
    "Here's an example of how you can implement the ETL process using Python:\n",
    "\n",
    "import csv\n",
    "import psycopg2\n",
    "\n",
    "# Extraction Step\n",
    "def extract_data(csv_file):\n",
    "    data = []\n",
    "    with open(csv_file, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            data.append(row)\n",
    "    return data\n",
    "\n",
    "# Transformation Step\n",
    "def transform_data(data):\n",
    "    transformed_data = []\n",
    "    for row in data:\n",
    "        # Apply business rules or calculations\n",
    "        transformed_row = {\n",
    "            'column1': row['column1'],\n",
    "            'column2': int(row['column2']) * 2,\n",
    "            'column3': row['column3'].upper()\n",
    "        }\n",
    "        transformed_data.append(transformed_row)\n",
    "    return transformed_data\n",
    "\n",
    "# Loading Step\n",
    "def load_data(data):\n",
    "    conn = psycopg2.connect(host='your_host', port='your_port',\n",
    "                            dbname='your_db', user='your_user', password='your_password')\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    for row in data:\n",
    "        # Load data into the data warehouse\n",
    "        cur.execute(\"INSERT INTO your_table (column1, column2, column3) VALUES (%s, %s, %s)\",\n",
    "                    (row['column1'], row['column2'], row['column3']))\n",
    "\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "# Main ETL Process\n",
    "def etl_process(csv_file):\n",
    "    # Extract data from the source\n",
    "    extracted_data = extract_data(csv_file)\n",
    "\n",
    "    # Transform the extracted data\n",
    "    transformed_data = transform_data(extracted_data)\n",
    "\n",
    "    # Load the transformed data into the data warehouse\n",
    "    load_data(transformed_data)\n",
    "\n",
    "# Usage example\n",
    "csv_file = 'your_csv_file.csv'\n",
    "etl_process(csv_file)\n",
    "\n",
    "In this example, we assume that you have a CSV file as the source system. The extract_data function reads the data from the CSV file and returns a list of dictionaries representing the rows. The transform_data function applies the desired transformations to the extracted data. Finally, the load_data function loads the transformed data into a PostgreSQL database table.\n",
    "\n",
    "Make sure to replace the placeholders (your_host, your_port, your_db, your_user, your_password, your_table) with the appropriate values specific to your environment and data warehouse setup.\n",
    "\n",
    "You can further enhance the ETL process by adding error handling, logging, or additional data transformations as per your requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cdd09b",
   "metadata": {},
   "source": [
    "## TOPIC: Dimensional Modeling and Schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5289546",
   "metadata": {},
   "source": [
    "**1.** Design a star schema for a university database, including a fact table for student\n",
    "enrollments and dimension tables for students, courses, and time. Implement the\n",
    "schema using a database of your choice.\n",
    "\n",
    "\n",
    "**2.** Write SQL queries to retrieve data from the star schema, including aggregations\n",
    "and joins between the fact table and dimension tables"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c794fc79",
   "metadata": {},
   "source": [
    "To design a star schema for a university database, including a fact table for student enrollments and dimension tables for students, courses, and time, you can follow these steps:\n",
    "\n",
    "1. Determine the Fact Table:\n",
    "\n",
    "Fact table name: student_enrollments_fact\n",
    "Fact table columns: enrollment_id (Primary Key), student_id (Foreign Key referencing students_dim), course_id (Foreign Key referencing courses_dim), time_id (Foreign Key referencing time_dim), enrollment_date, grade\n",
    "\n",
    "2. Design Dimension Tables:\n",
    "\n",
    "a. Students Dimension Table:\n",
    "\n",
    "student_id (Primary Key)\n",
    "student_name\n",
    "major\n",
    "graduation_year\n",
    "other relevant student attributes\n",
    "b. Courses Dimension Table:\n",
    "\n",
    "course_id (Primary Key)\n",
    "course_name\n",
    "course_code\n",
    "department\n",
    "other relevant course attributes\n",
    "\n",
    "c. Time Dimension Table:\n",
    "\n",
    "time_id (Primary Key)\n",
    "enrollment_date (Date)\n",
    "semester\n",
    "academic_year\n",
    "other relevant time attributes\n",
    "\n",
    "3. Implementing the Star Schema in a Database:\n",
    "\n",
    "To implement the star schema, you can use a database of your choice. Here's an example using PostgreSQL:\n",
    "\n",
    "-- Create the students_dim table\n",
    "CREATE TABLE students_dim (\n",
    "  student_id SERIAL PRIMARY KEY,\n",
    "  student_name VARCHAR(255),\n",
    "  major VARCHAR(255),\n",
    "  graduation_year INT\n",
    ");\n",
    "\n",
    "-- Create the courses_dim table\n",
    "CREATE TABLE courses_dim (\n",
    "  course_id SERIAL PRIMARY KEY,\n",
    "  course_name VARCHAR(255),\n",
    "  course_code VARCHAR(255),\n",
    "  department VARCHAR(255)\n",
    ");\n",
    "\n",
    "-- Create the time_dim table\n",
    "CREATE TABLE time_dim (\n",
    "  time_id SERIAL PRIMARY KEY,\n",
    "  enrollment_date DATE,\n",
    "  semester VARCHAR(255),\n",
    "  academic_year INT\n",
    ");\n",
    "\n",
    "-- Create the student_enrollments_fact table\n",
    "CREATE TABLE student_enrollments_fact (\n",
    "  enrollment_id SERIAL PRIMARY KEY,\n",
    "  student_id INT REFERENCES students_dim(student_id),\n",
    "  course_id INT REFERENCES courses_dim(course_id),\n",
    "  time_id INT REFERENCES time_dim(time_id),\n",
    "  enrollment_date DATE,\n",
    "  grade VARCHAR(2)\n",
    ");\n",
    "\n",
    "4. Writing SQL Queries to Retrieve Data:\n",
    "\n",
    "Here are some SQL queries to retrieve data from the star schema, including aggregations and joins between the fact table and dimension tables:\n",
    "\n",
    "a. Retrieve total enrollments by course and semester:\n",
    "\n",
    "SELECT c.course_name, t.semester, COUNT(*) AS total_enrollments\n",
    "FROM student_enrollments_fact f\n",
    "JOIN courses_dim c ON f.course_id = c.course_id\n",
    "JOIN time_dim t ON f.time_id = t.time_id\n",
    "GROUP BY c.course_name, t.semester;\n",
    "\n",
    "b. Retrieve the average grade by department and academic year:\n",
    "SELECT c.department, t.academic_year, AVG(CAST(f.grade AS FLOAT)) AS avg_grade\n",
    "FROM student_enrollments_fact f\n",
    "JOIN courses_dim c ON f.course_id = c.course_id\n",
    "JOIN time_dim t ON f.time_id = t.time_id\n",
    "GROUP BY c.department, t.academic_year;\n",
    "\n",
    "c. Retrieve student enrollment details for a specific course:\n",
    "SELECT s.student_name, c.course_name, t.enrollment_date, f.grade\n",
    "FROM student_enrollments_fact f\n",
    "JOIN students_dim s ON f.student_id = s.student_id\n",
    "JOIN courses_dim c ON f.course_id = c.course_id\n",
    "JOIN time_dim t ON f.time_id = t.time_id\n",
    "WHERE c.course_name = 'CourseName';\n",
    "\n",
    "These queries demonstrate how you can retrieve data from the star schema by joining the fact table (student_enrollments_fact) with the dimension tables (students_dim, courses_dim, time_dim). Adjust the table and column names, as well as the filtering conditions, to match your specific schema and data warehouse requirements.\n",
    "\n",
    "Remember to replace the placeholders ('CourseName') in the queries with actual values specific to your data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af29f0da",
   "metadata": {},
   "source": [
    "## TOPIC: Performance Optimization and Querying"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5175e2",
   "metadata": {},
   "source": [
    "**1.** Scenario: You need to improve the performance of your data loading process in\n",
    "the data warehouse. Write a Python script that implements the following\n",
    "optimizations:\n",
    "\n",
    "**a)** Utilize batch processing techniques to load data in bulk instead of individual\n",
    "row insertion.\n",
    "\n",
    "**b)** Implement multi-threading or multiprocessing to parallelize the data loading\n",
    "process.\n",
    "\n",
    "**c)** Measure the time taken to load a specific amount of data before and after\n",
    "implementing these optimizations"
   ]
  },
  {
   "cell_type": "raw",
   "id": "efc6869c",
   "metadata": {},
   "source": [
    "Python script that implements the mentioned optimizations for improving the performance of data loading in a data warehouse:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cd44b939",
   "metadata": {},
   "source": [
    "import time\n",
    "import threading\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Function to simulate data loading for a single row\n",
    "def load_data(row):\n",
    "    # Data loading logic for a single row\n",
    "    time.sleep(0.1)  # Simulating data loading time\n",
    "\n",
    "# Function to load data in bulk using batch processing\n",
    "def load_data_batch(rows):\n",
    "    # Data loading logic for a batch of rows\n",
    "    time.sleep(0.5)  # Simulating batch data loading time\n",
    "\n",
    "# Function to measure the time taken to load data\n",
    "def measure_loading_time(load_function, data, batch_size):\n",
    "    start_time = time.time()\n",
    "\n",
    "    if batch_size > 1:\n",
    "        # Perform batch processing using multiple threads or processes\n",
    "        num_batches = len(data) // batch_size\n",
    "        remaining_rows = len(data) % batch_size\n",
    "\n",
    "        if num_batches > 0:\n",
    "            for i in range(num_batches):\n",
    "                batch_data = data[i * batch_size : (i + 1) * batch_size]\n",
    "\n",
    "                if threading_enabled:\n",
    "                    # Using threading for parallel data loading\n",
    "                    threads = [threading.Thread(target=load_function, args=(row,)) for row in batch_data]\n",
    "                    for thread in threads:\n",
    "                        thread.start()\n",
    "                    for thread in threads:\n",
    "                        thread.join()\n",
    "                else:\n",
    "                    # Using multiprocessing for parallel data loading\n",
    "                    pool = Pool(processes=multiprocessing_processes)\n",
    "                    pool.map(load_function, batch_data)\n",
    "                    pool.close()\n",
    "                    pool.join()\n",
    "\n",
    "        if remaining_rows > 0:\n",
    "            remaining_data = data[-remaining_rows:]\n",
    "            load_function(remaining_data)\n",
    "    else:\n",
    "        # Load data row by row\n",
    "        for row in data:\n",
    "            load_function(row)\n",
    "\n",
    "    end_time = time.time()\n",
    "    loading_time = end_time - start_time\n",
    "    return loading_time\n",
    "\n",
    "# Sample data to load (replace with your actual data)\n",
    "data_to_load = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "# Configuration parameters\n",
    "batch_size = 3  # Number of rows to process in each batch\n",
    "threading_enabled = True  # Set to True to use threading, set to False to use multiprocessing\n",
    "multiprocessing_processes = 2  # Number of processes to use if multiprocessing is enabled\n",
    "\n",
    "# Measure loading time before optimizations\n",
    "loading_time_before = measure_loading_time(load_data, data_to_load, 1)\n",
    "\n",
    "# Measure loading time after optimizations\n",
    "loading_time_after = measure_loading_time(load_data_batch, data_to_load, batch_size)\n",
    "\n",
    "# Print the loading time\n",
    "print(f\"Loading time before optimizations: {loading_time_before} seconds\")\n",
    "print(f\"Loading time after optimizations: {loading_time_after} seconds\")\n",
    "\n",
    "===========================================================================================================================\n",
    "In this script, we have the load_data() function that simulates the data loading process for a single row. The load_data_batch() function simulates the bulk data loading process for a batch of rows.\n",
    "\n",
    "The measure_loading_time() function is used to measure the time taken to load data using a specified loading function (load_function). It takes into account the batch size and the choice of threading or multiprocessing for parallelization.\n",
    "\n",
    "We first measure the loading time before optimizations by calling measure_loading_time() with load_data and a batch size of 1.\n",
    "\n",
    "Then, we measure the loading time after optimizations by calling measure_loading_time() with load_data_batch and a specified batch size.\n",
    "\n",
    "The loading time before and after optimizations are printed to compare the performance improvement.\n",
    "\n",
    "Note that this script is a simplified example to demonstrate the concepts of batch processing and parallelization. You would need to modify the load_data() and load_data_batch() functions with your actual data loading logic. Additionally, you may need to adapt the script based on your specific requirements and data loading scenario.\n",
    "\n",
    "Remember to test and tune the batch size and number of threads or processes based on your system's capabilities and workload to achieve the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71857a46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
