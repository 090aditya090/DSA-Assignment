{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88bad5a0",
   "metadata": {},
   "source": [
    "**1.** Write a Python program to read a Hadoop configuration file and display the core\n",
    "components of Hadoop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2249df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configparser import ConfigParser\n",
    "\n",
    "def read_hadoop_config(config_file):\n",
    "    config = ConfigParser()\n",
    "    config.read(config_file)\n",
    "    core_components = config.sections()\n",
    "    return core_components\n",
    "\n",
    "# Example usage\n",
    "config_file = 'hadoop.conf'\n",
    "components = read_hadoop_config(config_file)\n",
    "print(\"Core Components of Hadoop:\")\n",
    "for component in components:\n",
    "    print(component)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e261be4",
   "metadata": {},
   "source": [
    "**2.** Implement a Python function that calculates the total file size in a Hadoop\n",
    "Distributed File System (HDFS) directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ab9cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def calculate_directory_size(directory):\n",
    "    command = \"hdfs dfs -du -s {}\".format(directory)\n",
    "    output = subprocess.check_output(command, shell=True).decode('utf-8').strip()\n",
    "    size = int(output.split()[0])\n",
    "    return size\n",
    "\n",
    "# Example usage\n",
    "directory = '/user/hadoop/data'\n",
    "total_size = calculate_directory_size(directory)\n",
    "print(\"Total file size in directory '{}': {} bytes\".format(directory, total_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba768d2a",
   "metadata": {},
   "source": [
    "**3.** Create a Python program that extracts and displays the top N most frequent words\n",
    "from a large text file using the MapReduce approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ebdabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from heapq import nlargest\n",
    "\n",
    "class TopNWords(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        for word in line.strip().split():\n",
    "            yield word, 1\n",
    "\n",
    "    def combiner(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "    def reducer_init(self):\n",
    "        self.top_n = 10\n",
    "        self.heap = []\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        total_count = sum(counts)\n",
    "        if len(self.heap) < self.top_n:\n",
    "            self.heap.append((total_count, word))\n",
    "        else:\n",
    "            min_count = min(self.heap)\n",
    "            if total_count > min_count[0]:\n",
    "                self.heap.remove(min_count)\n",
    "                self.heap.append((total_count, word))\n",
    "\n",
    "    def reducer_final(self):\n",
    "        top_words = nlargest(self.top_n, self.heap)\n",
    "        for count, word in top_words:\n",
    "            yield word, count\n",
    "\n",
    "# Example usage\n",
    "input_file = 'large_text_file.txt'\n",
    "mr_job = TopNWords(args=[input_file])\n",
    "top_words = mr_job.run()\n",
    "\n",
    "print(\"Top 10 most frequent words:\")\n",
    "for word, count in top_words:\n",
    "    print(word, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7fc29a",
   "metadata": {},
   "source": [
    "**4.** Write a Python script that checks the health status of the NameNode and DataNodes\n",
    "in a Hadoop cluster using Hadoop's REST API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91455802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def check_health_status():\n",
    "    nn_url = 'http://<namenode_hostname>:50070/jmx?qry=Hadoop:service=NameNode,name=NameNodeStatus'\n",
    "    dn_url = 'http://<datanode_hostname>:50075/jmx?qry=Hadoop:service=DataNode,name=FSDatasetState'\n",
    "\n",
    "    nn_response = requests.get(nn_url).json()\n",
    "    dn_response = requests.get(dn_url).json()\n",
    "\n",
    "    nn_status = nn_response['beans'][0]['State']\n",
    "    dn_status = dn_response['beans'][0]['VolumeInfo'][0]['FailedVolumes']\n",
    "\n",
    "    print(\"NameNode status:\", nn_status)\n",
    "    print(\"DataNode status:\", dn_status)\n",
    "\n",
    "# Example usage\n",
    "check_health_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04192dbc",
   "metadata": {},
   "source": [
    "**5.** Develop a Python program that lists all the files and directories in a specific HDFS\n",
    "path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e2d5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def list_hdfs_path(path):\n",
    "    command = \"hdfs dfs -ls -R {}\".format(path)\n",
    "    output = subprocess.check_output(command, shell=True).decode('utf-8').strip()\n",
    "    files = output.split('\\n')\n",
    "    for file_info in files:\n",
    "        print(file_info)\n",
    "\n",
    "# Example usage\n",
    "path = '/user/hadoop/data'\n",
    "print(\"Contents of HDFS path '{}':\".format(path))\n",
    "list_hdfs_path(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49999ea",
   "metadata": {},
   "source": [
    "**6.** Implement a Python program that analyzes the storage utilization of DataNodes in a\n",
    "Hadoop cluster and identifies the nodes with the highest and lowest storage capacities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19015901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def analyze_storage_utilization():\n",
    "    dn_url = 'http://<datanode_hostname>:50075/jmx?qry=Hadoop:service=DataNode,name=FSDatasetState'\n",
    "\n",
    "    dn_response = requests.get(dn_url).json()\n",
    "\n",
    "    volumes = dn_response['beans'][0]['VolumeInfo']\n",
    "    sorted_volumes = sorted(volumes, key=lambda x: x['usedSpace'], reverse=True)\n",
    "\n",
    "    print(\"DataNodes with highest storage capacities:\")\n",
    "    for volume in sorted_volumes[:5]:\n",
    "        print(\"Datanode:\", volume['datanodeInfo'])\n",
    "        print(\"Storage Capacity:\", volume['capacity'])\n",
    "        print(\"Used Space:\", volume['usedSpace'])\n",
    "        print(\"Free Space:\", volume['freeSpace'])\n",
    "        print()\n",
    "\n",
    "    print(\"DataNodes with lowest storage capacities:\")\n",
    "    for volume in sorted_volumes[-5:]:\n",
    "        print(\"Datanode:\", volume['datanodeInfo'])\n",
    "        print(\"Storage Capacity:\", volume['capacity'])\n",
    "        print(\"Used Space:\", volume['usedSpace'])\n",
    "        print(\"Free Space:\", volume['freeSpace'])\n",
    "        print()\n",
    "\n",
    "# Example usage\n",
    "analyze_storage_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c03ffa6",
   "metadata": {},
   "source": [
    "**7.** Create a Python script that interacts with YARN's ResourceManager API to submit\n",
    "a Hadoop job, monitor its progress, and retrieve the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682aafdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def submit_and_monitor_job():\n",
    "    submit_url = 'http://<resourcemanager_hostname>:8088/ws/v1/cluster/apps/new-application'\n",
    "    submit_response = requests.post(submit_url)\n",
    "    app_id = submit_response.json()['application-id']\n",
    "    print(\"Job submitted. Application ID:\", app_id)\n",
    "\n",
    "    # Submit your job using the obtained application ID\n",
    "    # ...\n",
    "\n",
    "    # Monitor job progress\n",
    "    while True:\n",
    "        status_url = 'http://<resourcemanager_hostname>:8088/ws/v1/cluster/apps/{}'.format(app_id)\n",
    "        status_response = requests.get(status_url)\n",
    "        status = status_response.json()['app']['state']\n",
    "        print(\"Job status:\", status)\n",
    "\n",
    "        if status == 'FINISHED':\n",
    "            break\n",
    "        elif status == 'FAILED':\n",
    "            print(\"Job failed.\")\n",
    "            return\n",
    "\n",
    "        time.sleep(10)  # Wait for 10 seconds before checking the status again\n",
    "\n",
    "    # Retrieve the final output of the job\n",
    "    output_url = 'http://<resourcemanager_hostname>:8088/ws/v1/cluster/apps/{}/finalStatus'.format(app_id)\n",
    "    output_response = requests.get(output_url)\n",
    "    final_output = output_response.json()['status']\n",
    "    print(\"Final output:\", final_output)\n",
    "\n",
    "# Example usage\n",
    "submit_and_monitor_job()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffc8a5e",
   "metadata": {},
   "source": [
    "**8.** Create a Python script that interacts with YARN's ResourceManager API to submit\n",
    "a Hadoop job, set resource requirements, and track resource usage during job\n",
    "execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365818c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def submit_and_track_resources():\n",
    "    submit_url = 'http://<resourcemanager_hostname>:8088/ws/v1/cluster/apps/new-application'\n",
    "    submit_response = requests.post(submit_url)\n",
    "    app_id = submit_response.json()['application-id']\n",
    "    print(\"Job submitted. Application ID:\", app_id)\n",
    "\n",
    "    # Set your resource requirements for the job\n",
    "    resource_request = {\n",
    "        \"application-id\": app_id,\n",
    "        \"resource\": {\n",
    "            \"memory\": 2048,\n",
    "            \"vCores\": 2\n",
    "        }\n",
    "    }\n",
    "    resource_url = 'http://<resourcemanager_hostname>:8088/ws/v1/cluster/apps/{}/resource-requests'.format(app_id)\n",
    "    requests.post(resource_url, json=resource_request)\n",
    "\n",
    "    # Submit your job using the obtained application ID\n",
    "    # ...\n",
    "\n",
    "    # Track resource usage during job execution\n",
    "    while True:\n",
    "        resource_usage_url = 'http://<resourcemanager_hostname>:8088/ws/v1/cluster/apps/{}/allocation'.format(app_id)\n",
    "        resource_response = requests.get(resource_usage_url)\n",
    "        resource_info = resource_response.json()\n",
    "\n",
    "        # Process and print the resource information\n",
    "        # ...\n",
    "\n",
    "        status_url = 'http://<resourcemanager_hostname>:8088/ws/v1/cluster/apps/{}'.format(app_id)\n",
    "        status_response = requests.get(status_url)\n",
    "        status = status_response.json()['app']['state']\n",
    "        print(\"Job status:\", status)\n",
    "\n",
    "        if status == 'FINISHED':\n",
    "            break\n",
    "        elif status == 'FAILED':\n",
    "            print(\"Job failed.\")\n",
    "            return\n",
    "\n",
    "        time.sleep(10)  # Wait for 10 seconds before checking the status and resource usage again\n",
    "\n",
    "# Example usage\n",
    "submit_and_track_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af088818",
   "metadata": {},
   "source": [
    "**9.** Write a Python program that compares the performance of a MapReduce job with\n",
    "different input split sizes, showcasing the impact on overall job execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5e7dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "import time\n",
    "\n",
    "class SplitSizeComparison(MRJob):\n",
    "\n",
    "    def configure_args(self):\n",
    "        super(SplitSizeComparison, self).configure_args()\n",
    "        self.add_passthru_arg('--split-size', type=int, default=64, help='Input split size in megabytes')\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        # Your mapper implementation\n",
    "        pass\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        # Your reducer implementation\n",
    "        pass\n",
    "\n",
    "# Example usage\n",
    "input_file = 'large_text_file.txt'\n",
    "split_sizes = [64, 128, 256, 512]\n",
    "\n",
    "print(\"Comparing MapReduce job performance with different input split sizes:\")\n",
    "\n",
    "for split_size in split_sizes:\n",
    "    start_time = time.time()\n",
    "\n",
    "    mr_job = SplitSizeComparison(args=[input_file, '--split-size', split_size])\n",
    "    with mr_job.make_runner() as runner:\n",
    "        runner.run()\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(\"Input Split Size: {} MB, Elapsed Time: {:.2f} seconds\".format(split_size, elapsed_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
